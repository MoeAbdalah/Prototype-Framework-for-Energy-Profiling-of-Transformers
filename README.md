# Prototype-Framework-for-Energy-Profiling-of-Transformers
 This project develops a framework to evaluate the energy consumption of transformer-based language models (LLMs) like BERT and GPT-2. Using the GLUE benchmark, we identify a nonlinear relationship between model performance and energy use, emphasizing the need for further research on query complexity and energy efficiency.
